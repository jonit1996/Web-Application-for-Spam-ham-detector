{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "''' Natural Language Processing is wide field as we know before,Here we are discussing about basic applications of NLP That is Email classification into spam and ham which is popular evry one.\n",
    "we will take up an extremely popular use case of NLP - building a supervised machine learning model on text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    " ''' Here we are going to automating to identify wheather the massage spam or ham. Nowdas spam messages are increasing randomly peoples are getting deffecult to identify wheather the message contains spam or not.So we are going to generate a classification model to classify wheather the message is ham or spam.\n",
    " '''We will try to address this problem by building a text classification model which will automate the process.\n",
    " The dataset we will use comes from Kaggle sample data set for LSTM for text classification Spam.csv and contains\n",
    " 5572 observations and 5 variables, as described below\n",
    " \n",
    "   v1 : Indicating wheather message is spam or ham \n",
    "   \n",
    "   v2 : messages\n",
    "   \n",
    "   v3 : Non\n",
    "   \n",
    "   v4 : Non \n",
    "   \n",
    "   v5 : Non \n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the required libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data and performing basic data checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv',delimiter=',',encoding='latin-1')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "v1    5572 non-null object\n",
      "v2    5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"  Let us check the distribution of the target class which can be done using barplot.groups the 'V1' variables by counting     the number of their occurrences.\n",
    "It is evident that we have more occurrences of 'ham' than 'spam' in the target variable. Still, the good thing is that the difference is not significant and the data is relatively balanced.\n",
    "\n",
    "\n",
    "The baseline accuracy is important but often ignored in machine learning. It sets the benchmark in terms of minimum accuracy which the model should achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5179468772433596\n"
     ]
    }
   ],
   "source": [
    "df.groupby('v1').v2.count().plot.bar(ylim=0)\n",
    "plt.show()\n",
    "print(2886/5572) #Baseline accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5179468772433596 # Baseline accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the Raw Text and Getting It Ready for Machine Learning\n",
    "\n",
    "Now, we are ready to build our text classifier. However, this is where things begin to get trickier in NLP. The data we have is in raw text which by itself, cannot be used as features. So, we will have to pre-process the text.\n",
    "\n",
    "\n",
    "For completing the above-mentioned steps, we will have to load the nltk package, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jonit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With nltk package loaded and ready to use, we will perform the pre-processing tasks. The first two lines of code below imports the stopwords and the PorterStemmer modules, respectively.\n",
    "\n",
    "The third line imports the regular expressions library, ‘re’, which is a powerful python package for text parsing. To learn more about text parsing and the 're' library, please refer to the guide'Natural Language Processing – Text Parsing'(/guides/text-parsing).\n",
    "\n",
    "The fourth to sixth lines of code does the text pre-processing discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = stopwords.words(\"english\")\n",
    "\n",
    "df['processedtext'] = df['v2'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).\n",
    "                                                         split() if i not in words]).upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at the pre-processed data set that has a new column 'processedtext'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>processedtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>GO JURONG POINT CRAZI AVAIL BUGI N GREAT WORLD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>OK LAR JOKE WIF U ONI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>FREE ENTRI WKLI COMP WIN FA CUP FINAL TKT ST M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U DUN SAY EARLI HOR U C ALREADI SAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NAH I THINK GOE USF LIVE AROUND THOUGH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       processedtext  \n",
       "0  GO JURONG POINT CRAZI AVAIL BUGI N GREAT WORLD...  \n",
       "1                              OK LAR JOKE WIF U ONI  \n",
       "2  FREE ENTRI WKLI COMP WIN FA CUP FINAL TKT ST M...  \n",
       "3                U DUN SAY EARLI HOR U C ALREADI SAY  \n",
       "4             NAH I THINK GOE USF LIVE AROUND THOUGH  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  spliting train data sand test data \n",
    "\n",
    "We have already imported necessory pakages to split our data into train and test.\n",
    "\n",
    "we are creating an array of the target variable, called 'target'.\n",
    "\n",
    "\n",
    "Then we are creating the training (X_train, y_train) and test set (X-test, y_test) arrays. It keeps 30% of the data for testing the model. The 'random_state' argument ensures that the results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 3)\n",
      "(3900,)\n",
      "(1672,)\n"
     ]
    }
   ],
   "source": [
    "target = df['v1']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['processedtext'], target, test_size=0.30, random_state=100)\n",
    "\n",
    "print(df.shape); print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Text to Word Frequency Vectors with TfidfVectorizer.\n",
    "\n",
    "We have processed the text, but we need to convert it to word frequency vectors for building machine learning models. There are several ways to do this, such as using CountVectorizer and HashingVectorizer, but the TfidfVectorizer is the most popular one.\n",
    "\n",
    "TF-IDF is an acronym that stands for 'Term Frequency-Inverse Document Frequency'. It is used as a weighting factor in text mining applications.\n",
    "\n",
    "\n",
    "Term Frequency (TF): This summarizes the normalized Term Frequency within a document.\n",
    "\n",
    "\n",
    "Inverse Document Frequency (IDF): This reduces the weight of terms that appear a lot across documents. In simple terms, TF-IDF attempts to highlight important words which are frequent in a document but not across documents. We will work on creating TF-IDF vectors for our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aah', 'aaniy', 'aaooooright', 'aathi', 'abbey', 'abeg', 'abel', 'aberdeen', 'abi', 'abil']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "\n",
    "train_tfIdf = vectorizer_tfidf.fit_transform(X_train.values.astype('U'))\n",
    "\n",
    "test_tfIdf = vectorizer_tfidf.transform(X_test.values.astype('U'))\n",
    "\n",
    "print(vectorizer_tfidf.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the shape of the transformed TF-IDF train and test datasets. The following line of code performs this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900, 5139)\n",
      "(1672, 5139)\n"
     ]
    }
   ],
   "source": [
    "print(train_tfIdf.shape); print(test_tfIdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit the Classifier.\n",
    "\n",
    "\n",
    "Now, we will build the text classification model. The algorithm we will choose is the Naive Bayes Classifier, which is commonly used for text classification problems\n",
    "\n",
    "\n",
    "Finally, our model is trained and it is ready to generate predictions on the unseen data. This is performed in the fifth line of code, while the sixth line prints the predicted class for the first 10 records in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(train_tfIdf, y_train)\n",
    "\n",
    "pred2 = nb_classifier.predict(test_tfIdf) \n",
    "print(pred2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Evaluation Metrics\n",
    "\n",
    "We are now ready to evaluate the performance of our model on test data. Using the 'metrics.accuracy_score’ function, we compute the accuracy in the first line of code below and print the result using the second line of code. We see that the accuracy is 96.8%, which is a good score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9683014354066986\n",
      "[[1458    1]\n",
      " [  52  161]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score: score\n",
    "accuracy_tfidf = metrics.accuracy_score(y_test, pred2)\n",
    "print(accuracy_tfidf)\n",
    "\n",
    "Conf_metrics_tfidf = metrics.confusion_matrix(y_test, pred2, labels=['ham', 'spam'])\n",
    "print(Conf_metrics_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Naïve Bayes Model\n",
    "\n",
    "At the beginning of the guide, we established the baseline accuracy of 51.8%. Our Naive Bayes model is conveniently beating this baseline model by achieving the accuracy score of 96.8%. This also sets a new benchmark for any new algorithm or model refinements. We will try out the Random Forest Algorithm to see if it improves our result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=50, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 50)\n",
    "\n",
    "classifier.fit(train_tfIdf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "predRF = classifier.predict(test_tfIdf) \n",
    "print(predRF[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972488038277512\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "accuracy_RF = metrics.accuracy_score(y_test, predRF)\n",
    "print(accuracy_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy is increased to 97.5, Compairing to Naive Bayes Model nothing that much change any way both models give us a good score for this classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1457    2]\n",
      " [  44  169]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Conf_metrics_RF = metrics.confusion_matrix(y_test, predRF, labels=['ham', 'spam'])\n",
    "print(Conf_metrics_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this guide, you have learned the fundamentals of text cleaning and pre-processing, and building and evaluating text classification models using Naive Bayes and Random Forest Algorithms. The performance of the models is summarized below:\n",
    "\n",
    " \n",
    " 1 : Baseline Model Accuracy - 51.8%\n",
    " \n",
    " 2 : Accuracy achieved by Naive Bayes Classifier  - 96.8% \n",
    " \n",
    " 3 : Accuracy achieved by Random Forest Classifier -97.2\n",
    " \n",
    " \n",
    " We can see our both algorthams are easly beats the baseline accuracy and also our both algorithams have almost approximately  similar accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
